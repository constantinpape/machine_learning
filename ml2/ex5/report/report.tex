\documentclass{article}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{bbold}

\begin{document}

Machine Learning 2, exercise 5 by Constantin Pape, Marcus Theisen and Johann Klaehn
 
\section{Exercise and data}

In this exercise we use Gaussian Processes for paramater optimization.
To this end, we use a first Gaussian Process to regress an image and then use a second Gaussian Process to find the best hyperparameters for the first GP.

\section{Kernels}

We implement two different Kernels: 
A modified exponential kernel, that depends on the paramaters $\rho$ and $\gamma$, for the 
interpolation and a Matern kernel, that depends on $\sigma_\rho$, $\sigma_\gamma$  and $\sigma_\tau$ for learning hyperparamers.

To test the modified exponential kernel. we reconstruct the given image with the parameters
$\tau$ = 0.8, $\rho$ = 7.5, $\gamma$ = 1.8.
With this paramers, we achieve a mean squared pixel error of 350.86. 
The resulting image is shown in \autoref{fig1}.

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{../res_1.png}
	\caption{Reconstruction with GP.}
	\label{fig1}
\end{figure}

\section{Speedup}

To speedup the computation of the Gaussian Process we stick to the implementation provided.
\newline
In this two features provide a speedup:
\begin{itemize}
    \item The inversion of the kernel is not done explicitly. Instead the linear system 
        $\mathbf{\tilde{y}} = (K + \tau \mathbb{1})^{-1}$ is solved for $\mathbf{\tilde{y}}$.
    \item A KDTree is used in the prediction step for finding datapoints that are inside
        the maximum distance to the query point.
        A KDTree is a binary tree with axis-aligned splits, that speeds up neighbor search for 
        low dimensions significantly.
\end{itemize}

\section{Bayesian Optimization of Hyperparameters}

In the next step we try to optimize the hyperparamers of the first Gaussian process with a second gaussian 
process, using the Matern Kernel. 
For this we generated a Sobol sequence of 2000 points in the hyperparameter space $\tau = 0.1 ... 0.9$, $\rho = 2 ... 10$, $\gamma = 1 ... 4$.
Then we ran the first Gaussian process on the 20 first points of this sobol sequence to generate training points and then ran the second gaussian process 20 times on these test points.
For comparison we ran only the first Gaussian process 40 times on random points of the sobol sequence.
Unfortunately the second Gaussian process produces negative predictions (couldnt find the reason for this).
Due to this fact we tried two different sampling strategies:
\begin{itemize}
    \item The strategy suggested on the exercise sheet: Taking the minimum of $MSE / \sqrt{VAR}$ (where we had to exclude NAN values due to negative values in the sqrt)
    \item Taking the minimum of $\textrm{ABS}({MSE / VAR})$, in the hope that small absolute values correspond to a good precast.
\end{itemize}
In table \autoref{tab1} the best results for all three methods are listed.

\begin{table}[h]
	\centering
	\begin{tabular}{l c c c c}
        Method	&	$\tau$	& $\rho$  & $\gamma$&   $MSE$	\\
        Random	&	0.18	& 6.78    & 1.08    &	321.60  \\
        GP2 1	&	0.26	&         & 1.71    &	322.35  \\
        GP2 2	&	0.22	& 6.89    & 1.24    &	318.93  \\
	\end{tabular}
	\caption{Test classification after training.}
	\label{tab1}
\end{table}

Apperently the results for all three methods do not differ significantly.
We further investigated the evolution of the MSE over the iterations (see \autoref{tab2}).
Here we see that the spread in the MSEs is smaller for the Gaussian process solutions than for the random method. 
Hence the Gaussian process 2 seems to do better than random search although it produces negative MSE predictions.
Also the Method 2 seems to produce better sequences than method 1.

\begin{table}[h]
	\centering
	\begin{tabular}{l c c c}
        Iteration&	Rand	& GP2 1    & GP2 2    \\
           1     &   367    &439	   & 341      \\
           2     &   47206  &42144	   & 355      \\
           3     &   67618  &119078	   & 349      \\
           4     &   106113 &13332     & 343      \\
           5     &   361    &346       & 327      \\
           6     &   1002   &361       & 339      \\
           7     &   167003 &290684    & 15670    \\
           8     &   378    &19387     & 368      \\
           9     &   542662 &340       & 359      \\
          10     &   363    &477       & 320      \\
          11     &   477    &404       & 56354    \\
          12     &   40333  &425       & 318      \\
          13     &   106113 &341       & 382      \\
          14     &   509    &70220     & 818      \\
          15     &   342    &328       & 8394     \\
          16     &   516    &20231     & 376      \\
          17     &   735    &323       & 2868416  \\
          18     &   54371  &322       & 341      \\
          19     &   353    &860071    & 365      \\
          20     &   27211  &9734      & 407      \\
	\end{tabular}   
	\caption{Test classification after training.}
	\label{tab2}
\end{table}

Finally we plot the best interpolations (see \autoref{fig2} to \autoref{fig4}).

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{../res_2.png}
	\caption{Reconstruction with best result of Random Optimization.}
	\label{fig2}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{../res_3.png}
	\caption{Reconstruction with best result of GP Optimization, method 1.}
	\label{fig3}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{../res_4.png}
	\caption{Reconstruction with best result of GP Optimization, method 2.}
	\label{fig4}
\end{figure}

\end{document}
