\documentclass{article}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{subcaption}

\begin{document}

Machine Learning 2, exercise 4 by Constantin Pape, Marcus Theisen and Johann Klaehn
 
\section{Exercise and data}

In this exercise we implement different Multi-Class Classifiers.
All of them are based on the RandomForest from sklearn, which is configured with 20 trees.
We use the digits dataset from sklearn for evaluating the classifiers.
Before using it, we normalize the data.
In the end all classifiers are compared via crossvalidation (see \autoref{res})

\section{One Vs Rest}

First we implement an One-vs-Rest-Classifier. In this we train a classifier for each class, treating the class as positive class and all other classes as negative example.
\newline
We implement two different algorithms for prediction:
\begin{itemize}
    \item Argmax prediction: Predicting the class with the highest probability.
    \item Binary prediction: Letting each classifier predict individually. If only one classifier has predicted a positive class, return this, otherwise return unknown.
\end{itemize}
To measure the effect of the uneven ratio of negative and positive samples, we try a classifier with weights adjusted to the ratio of positive to negative samples. 
This is done by the flag "class\_weight = auto" for the sklearn Random Forest.
See \autoref{res} for the performance of the different implementations.

\section{Error-Correcting}

We implement an Error-Correcting-Classifier. This is done by using a binary code word of length P for each class and then training P classifiers, each on one letter in the code word.
The code word is generated by taking the eigenvectors of the training data correlation matrix.
For P = 9 we delete the eigenvector with the smallest eigenvalue, for P = 11 we add a random column.
Alternatively a random code matrix is generated.
First we investigate the maximal achievable distance between the code words.
The theoretical maximum is given by:

\begin{equation}
    d = P - Q + 1 
\end{equation}

where Q is the log of the number of class (here Q = 4).
The experimental code distance is calculated by:

\begin{equation}
    d = \frac{1}{2} (P - \textrm{max}(C \cdot C^{T} - P \cdot I) )
\end{equation}

See \autoref{tab1} for the results. 

\begin{table}[h]
	\centering
	\begin{tabular}{l c c c c}
        P	            &	9	& 10    &   11   \\
        Theoretical	    &	6	&  7    &    8   \\
        Random Code     &   2   &  1    &    2   \\
        Correlation Code&   0   &  0    &    0   \\ 
	\end{tabular}
	\caption{Table of the maximum code distance. Showing the theoretical maximum and the values for both codes. All calculations on the full data.}
	\label{tab1}
\end{table}

In the prediction step, the class is predicted, which code word has the lowest hamming distance to the word generate for the new instance.
A discussion of the results is given in \autoref{res}.

\section{One Vs One}

We implement a One-Vs-One-Classifier. In this each classifier is only trained on 2 classes.
Two different approaches are implemented:
\begin{itemize}
    \item Pairwise: A classifier is trained for each pair of classes.
    \item Greedy: A classifier is trained only on sequential classes. 
\end{itemize}

\section{Multi-Class RandomForest}

For comparison, we also train a multi-class RandomForest on the data. 

\section{Results} \label{res} 

All classifiers are tested on the dataset via 10-fold cross validation.
\autoref{tab2} shows the results for the One-vs-One Classifier.
As can be seen the Argmax prediction scores better than the Binary prediction.
The Argmax prediction profits from the adjusted weights, the Binary prediction doesnt.

\begin{table}[h]
	\centering
	\begin{tabular}{l l c}
        Prediction Method   & Adjusted Weights  & Classification Rate   \\
        Argmax	            & False             &   0.9366 		        \\
                            & True              &   0.9506              \\
        Binary	            & False             &	0.8202              \\
                            & True              &   0.7978              \\
	\end{tabular}
	\caption{Results for the One-Vs-Rest Classifier.}
	\label{tab2}
\end{table}

The results for the Error-Correcting Classifier are shown in \autoref{tab2}.
The classifier performs significantly better with the random code than with the
code based on the correlation matrix.
This fact is in correspondance with the maximum code distance measured before 
(see \autoref{tab1}), that is also greater for the random code.
The influence of the code length is not significant for the random code.
For the correlation code, the classification rate increases significantly with P = 11.
However this can be accounted to the fact, that we add a random column to the code matrix.

\begin{table}[h]
	\centering
	\begin{tabular}{l c c c}
        P                   & 9     & 10    & 11    \\
        Correlation Code	& 0.5806&0.6149 &0.7177 \\         
        Random Code	        & 0.9054&0.8964 &0.9226 \\	
    \end{tabular}
	\caption{Results for the Error-Correcting Classifier.}
	\label{tab3}
\end{table}

The results for the One-vs-One Classifier are shown in \autoref{tab4}.
The pairwise method performs significantly better than the greedy method.

\begin{table}[h]
	\centering
	\begin{tabular}{l c}
        Method      &  Classification rate   \\
        Pairwise    &  0.9383    		     \\
        Greedy      &  0.7507                \\
	\end{tabular}
	\caption{Results for the One-vs-Rest Classifier.}
	\label{tab4}
\end{table}

\autoref{tab5} shows the results for the Multi-Class Random Forest.
Overall it is the best performing method. It is closely followed by the One-Vs-Rest Classifier with Argmax Prediction and adjusted weights (the difference is not significant). Also the pairwise One-Vs-One Classifier and the Error-Correcting Classifier with random code perform quite well. 
The worst performing method is the Error-Correcting Classifier with correlation code.

\begin{table}[h]
	\centering
	\begin{tabular}{l c c c}
        Classification Rate \\
        0.9539              \\
    \end{tabular}
	\caption{Results for the Multi-Class Random Forest.}
	\label{tab5}
\end{table}

\end{document}
